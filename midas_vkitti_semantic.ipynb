{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\email/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\email/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "model_type = \"MiDaS_small\"\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figuring out the self supervised training loop with pose estimation is hard, there are a bunch of different papers that implement their own version of it\n",
    "stil working on getting that set up\n",
    "meanwhile:\n",
    "\n",
    "make rgb depth dataloader\n",
    "write rgb-depth training loop\n",
    "\n",
    "make rgb semantic dataloader\n",
    "write rgb-semantic training loop\n",
    "\n",
    "figure out where to split model into encoder and decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\email\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\email\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Using cache found in C:\\Users\\email/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_type, path1, path2):\n",
    "        internal_image_path_1 = os.path.join(r'Scene01\\15-deg-left\\frames', path1.split('_')[-1], 'Camera_0')\n",
    "        internal_image_path_2 = os.path.join(r'Scene01\\15-deg-left\\frames', path2.split('_')[-1], 'Camera_0')\n",
    "        self.path1 = os.path.join(path1, internal_image_path_1)\n",
    "        self.path2 = os.path.join(path2, internal_image_path_2)\n",
    "        \n",
    "        file_map1 = {}\n",
    "        for f in os.listdir(self.path1):\n",
    "            try:\n",
    "                index = int(f.split('_')[1].split('.')[0])\n",
    "            except:\n",
    "                print(f)\n",
    "            file_map1[index] = (os.path.join(self.path1, f))\n",
    "        self.file_map1 = file_map1\n",
    "\n",
    "        file_map2 = {}\n",
    "        for f in os.listdir(self.path2):\n",
    "            try:\n",
    "                index = int(f.split('_')[1].split('.')[0])\n",
    "            except:\n",
    "                print(f)\n",
    "            file_map2[index] = (os.path.join(self.path2, f))\n",
    "        self.file_map2 = file_map2\n",
    "\n",
    "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "        if model_type == 'DPT_large':\n",
    "            self.transform = midas_transforms.dpt_transform \n",
    "        elif model_type == 'MiDaS_small':\n",
    "            self.transform = midas_transforms.small_transform \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # rgb input\n",
    "        img1 = cv2.imread(self.file_map1[i])\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        rgb = self.transform(img1) # already has batch dimension\n",
    "\n",
    "        img2 = Image.open(self.file_map2[i])\n",
    "        convert_tensor = v2.Compose(\n",
    "            [\n",
    "                v2.Resize(rgb.shape[-2:]),\n",
    "                v2.ToImageTensor()\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        depth = convert_tensor(img2)[None,:,:,:]\n",
    "\n",
    "        return rgb, depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.path1))\n",
    "\n",
    "depth = r'D:\\scalable_analytics_institute\\semantic_depth_transformer\\data\\virtual_kitti\\vkitti_2.0.3_depth'\n",
    "rgb = r'D:\\scalable_analytics_institute\\semantic_depth_transformer\\data\\virtual_kitti\\vkitti_2.0.3_rgb'\n",
    "semantic = r'D:\\scalable_analytics_institute\\semantic_depth_transformer\\data\\virtual_kitti\\vkitti_2.0.3_classSegmentation'\n",
    "\n",
    "rgb_semantic_dataset = Dataset(model_type, rgb, semantic)\n",
    "\n",
    "# the transforms provided by the midas torch hub already add the batch dimension to the image, so we need to define the collate function\n",
    "# in the dataloader manually to accomodate for this.\n",
    "def collate(batch):\n",
    "    return torch.cat([item[0] for item in batch], dim = 0), torch.cat([item[1] for item in batch], dim = 0)\n",
    "\n",
    "rgb_semantic_dataloader = DataLoader(rgb_semantic_dataset, batch_size=32, collate_fn=collate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 256])\n",
      "torch.Size([1, 3, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "# verify sizes of the images are correct\n",
    "print(rgb_semantic_dataset[0][0].shape)\n",
    "print(rgb_semantic_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 256])\n",
      "torch.Size([32, 3, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "# verify that the batches have the correct dimension\n",
    "x = next(iter(rgb_semantic_dataloader))\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\scalable_analytics_institute\\semantic_depth_transformer\\midas_vkitti_semantic.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m semantic \u001b[39m=\u001b[39m semantic\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)  \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted_semantic_map\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m predicted_semantic_map \u001b[39m-\u001b[39;49m semantic\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# prediction = torch.nn.functional.interpolate(\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     predicted_depth.unsqueeze(1),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#     size=img.shape[:2],\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#     mode=\"bicubic\",\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#     align_corners=False,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# ).squeeze()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/scalable_analytics_institute/semantic_depth_transformer/midas_vkitti_semantic.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m output \u001b[39m=\u001b[39m rgb[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,:,:]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "scale = lambda x : (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (rgb, semantic) in enumerate(rgb_semantic_dataloader):\n",
    "        rgb, semantic = rgb.to(device), semantic.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_semantic_map = model(rgb)\n",
    "        semantic = semantic.squeeze(1)  \n",
    "        print(predicted_semantic_map.shape)\n",
    "        x = predicted_semantic_map - semantic\n",
    "\n",
    "        # prediction = torch.nn.functional.interpolate(\n",
    "        #     predicted_depth.unsqueeze(1),\n",
    "        #     size=img.shape[:2],\n",
    "        #     mode=\"bicubic\",\n",
    "        #     align_corners=False,\n",
    "        # ).squeeze()\n",
    "\n",
    "\n",
    "        output = rgb[0,0,:,:].detach().cpu().numpy()\n",
    "        output = scale(output)\n",
    "        print(np.min(output))\n",
    "        print(np.max(output))\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        output = predicted_semantic_map[0,:,:].detach().cpu().numpy()\n",
    "        output = 1 - scale(output)\n",
    "        print(np.min(output))\n",
    "        print(np.max(output))\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        output = semantic[0,:,:].detach().cpu().numpy()\n",
    "        output = scale(output)\n",
    "        print(np.min(output))\n",
    "        print(np.max(output))\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        break\n",
    "    break\n",
    "        \n",
    "        # loss = criterion(predicted_depth, depth_gt)\n",
    "\n",
    "        # # Backpropagation and optimization\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # total_loss += loss.item()\n",
    "\n",
    "        # if (batch_idx + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # # Print average loss for the epoch\n",
    "    # average_loss = total_loss / len(dataloader)\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {average_loss}\")\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), \"./models/depth_estimation_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
