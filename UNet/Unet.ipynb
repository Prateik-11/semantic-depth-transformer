{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unet import UNet\n",
    "\n",
    "model = UNet(\n",
    "    dim=64   # dimension that will get multiplied by dim_mults\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_type, path1, path2):\n",
    "        internal_image_path_1 = os.path.join(r'Scene01\\15-deg-left\\frames', path1.split('_')[-1], 'Camera_0')\n",
    "        internal_image_path_2 = os.path.join(r'Scene01\\15-deg-left\\frames', path2.split('_')[-1], 'Camera_0')\n",
    "        self.path1 = os.path.join(path1, internal_image_path_1)\n",
    "        self.path2 = os.path.join(path2, internal_image_path_2)\n",
    "        \n",
    "        file_map1 = {}\n",
    "        for f in os.listdir(self.path1):\n",
    "            try:\n",
    "                index = int(f.split('_')[1].split('.')[0])\n",
    "            except:\n",
    "                print(f)\n",
    "            file_map1[index] = (os.path.join(self.path1, f))\n",
    "        self.file_map1 = file_map1\n",
    "\n",
    "        file_map2 = {}\n",
    "        for f in os.listdir(self.path2):\n",
    "            try:\n",
    "                index = int(f.split('_')[1].split('.')[0])\n",
    "            except:\n",
    "                print(f)\n",
    "            file_map2[index] = (os.path.join(self.path2, f))\n",
    "        self.file_map2 = file_map2\n",
    "\n",
    "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "        if model_type == 'DPT_large':\n",
    "            self.transform = midas_transforms.dpt_transform \n",
    "        elif model_type == 'MiDaS_small':\n",
    "            self.transform = midas_transforms.small_transform \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # rgb input\n",
    "        img1 = cv2.imread(self.file_map1[i])\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        rgb = self.transform(img1) # already has batch dimension\n",
    "\n",
    "        img2 = Image.open(self.file_map2[i])\n",
    "        convert_tensor = v2.Compose(\n",
    "            [\n",
    "                v2.Resize(rgb.shape[-2:]),\n",
    "                v2.ToImageTensor()\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        depth = convert_tensor(img2)[None,:,:,:]\n",
    "\n",
    "        return rgb, depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.path1))\n",
    "\n",
    "depth = r'D:\\scalable_analytics_institute\\semantic_depth_transformer\\data\\virtual_kitti\\vkitti_2.0.3_depth'\n",
    "rgb = r'D:\\scalable_analytics_institute\\semantic_depth_transformer\\data\\virtual_kitti\\vkitti_2.0.3_rgb'\n",
    "semantic = r'D:\\scalable_analytics_institute\\semantic_depth_transformer\\data\\virtual_kitti\\vkitti_2.0.3_classSegmentation'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
