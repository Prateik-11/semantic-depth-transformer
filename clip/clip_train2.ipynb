{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from transformers import CLIPImageProcessor, CLIPModel, CLIPTokenizer\n",
    "from tqdm.autonotebook import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "device = \"cuda:2\"\n",
    "batch_size = 32\n",
    "model_ID = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_ID)\n",
    "preprocess = CLIPImageProcessor.from_pretrained(model_ID)\n",
    "preprocess.do_center_crop = False # replaced with random crop in the dataloader\n",
    "preprocess.do_resize = False # handled in the dataloader\n",
    "\n",
    "# preprocess1 = CLIPImageProcessor.from_pretrained(model_ID)\n",
    "## turn off pre-processing to diagnose issues with the images looking wrong\n",
    "# preprocess1.do_center_crop = False\n",
    "# preprocess1.do_normalize = False\n",
    "# preprocess1.do_rescale = False\n",
    "# preprocess1.do_resize = False\n",
    "# preprocess1.do_rample = False\n",
    "# preprocess1.do_convert_rgb = True\n",
    "# preprocess2 = CLIPImageProcessor.from_pretrained(model_ID)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, depth_path, segmentation_path):\n",
    "        depth_paths = []\n",
    "        seg_paths = []\n",
    "\n",
    "        for root, dirs, files in os.walk(depth_path):\n",
    "            if files != []:    \n",
    "                depth_paths += [f'{root}/{file}' for file in files]\n",
    "                seg_paths += [f\"{root.replace('depth', 'classSegmentation')}/{file.replace('depth', 'classgt')}\" for file in files]\n",
    "        \n",
    "        ## since it searches alphabetically, these paths will not align with the paths of the other images by default\n",
    "        # segmentation_paths = []\n",
    "        # for root, dirs, files in os.walk(segmentation_path):\n",
    "        #     if files != []:    \n",
    "        #         segmentation_paths += [f'{root}/{file}' for file in files]\n",
    "        \n",
    "        self.depths = depth_paths\n",
    "        self.segs = seg_paths\n",
    "    \n",
    "    def convert_to_rgb(self, y, show = False):\n",
    "        y = torchvision.transforms.PILToTensor()(y)\n",
    "        y2 = (y - torch.min(y))\n",
    "        y3 = ((y2 / torch.max(y2)) * 255).type(torch.uint8)\n",
    "        y4 = torch.cat([y3, y3, y3], dim=0)\n",
    "        y5 = torchvision.transforms.ToPILImage()(y4)\n",
    "        if show:\n",
    "            display(y5)\n",
    "        return y5\n",
    "    \n",
    "    # def load_and_preprocess_image_depth(self, image_path):\n",
    "    #     # the PIL convert to rgb function does not work for the depth PNGs\n",
    "    #     # they will turn into one flat color\n",
    "    #     image = convert_to_rgb(Image.open(image_path))\n",
    "    #     image = preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]        \n",
    "    #     return image\n",
    "    \n",
    "    # def load_and_preprocess_image_seg(self, image_path):\n",
    "    #     image = Image.open(image_path)\n",
    "    #     image = preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    #     return image\n",
    "    \n",
    "    def load_and_preprocess(self, depth_path, seg_path):\n",
    "        depth = convert_to_rgb(Image.open(depth_path))\n",
    "        seg = Image.open(seg_path)\n",
    "\n",
    "        # random cropping\n",
    "        i, j, h, w = torchvision.transforms.RandomCrop.get_params(depth, output_size=(224, 224))\n",
    "        depth = torchvision.transforms.functional.crop(depth, i, j, h, w)\n",
    "        seg = torchvision.transforms.functional.crop(seg, i, j, h, w)\n",
    "        \n",
    "        # random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            depth = torchvision.transforms.functional.hflip(depth)\n",
    "            seg = torchvision.transforms.functional.hflip(seg)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            depth = torchvision.transforms.functional.vflip(depth)\n",
    "            seg = torchvision.transforms.functional.vflip(seg)\n",
    "        \n",
    "        depth = preprocess(depth, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        seg = preprocess(seg, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "        return depth, seg\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.load_and_preprocess(self.depths[i], self.segs[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.depths)\n",
    "\n",
    "dataset = Dataset(\n",
    "    depth_path = '/localhome/prateiksinha/kitti/depth',\n",
    "    segmentation_path = '/localhome/prateiksinha/kitti/classSegmentation'\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checking dataset:\n",
    "\n",
    "# i = 0\n",
    "# j = 446\n",
    "# x, y = dataset[i]\n",
    "# a, b = dataset[j]\n",
    "# import torchvision.transforms as T\n",
    "# transform = T.ToPILImage()\n",
    "# # print(torch.all(x == a))\n",
    "# # print(torch.all(y == b))\n",
    "# display(transform(a[0,:,:,:]))\n",
    "# display(transform(x[0,:,:,:]))\n",
    "# display(transform(b[0,:,:,:]))\n",
    "# display(transform(y[0,:,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(torch.nn.Module):\n",
    "    def __init__(self, model, projection, logit_scale):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.projection = projection\n",
    "        self.logit_scale = logit_scale\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.projection(self.model(x)['pooler_output'])\n",
    "        y = self.projection(self.model(y)['pooler_output'])\n",
    "\n",
    "        # normalize features\n",
    "        x = x / x.norm(p=2, dim=-1, keepdim=True)\n",
    "        y = y / y.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_y = torch.matmul(y, x.t()) * logit_scale\n",
    "        logits_per_x = logits_per_y.t()\n",
    "\n",
    "        return {\n",
    "            'logits per x': logits_per_x,\n",
    "            'logits per y': logits_per_y,\n",
    "            'x_features' : x,\n",
    "            'y_features' : y,\n",
    "        }\n",
    "\n",
    "image_model = ImageModel(model.vision_model, model.visual_projection, model.logit_scale)\n",
    "image_model = image_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB = False\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"depth-seg-CLIP\",\n",
    "        config={\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"architecture\": \"CLIP\",\n",
    "        \"dataset\": \"Virtual Kitti\",\n",
    "        \"epochs\": 10,\n",
    "        }\n",
    "    )\n",
    "\n",
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n",
    "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "optimizer = torch.optim.Adam(image_model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "\n",
    "epochs = 10\n",
    "save_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd8a231a4744201abdda09701ab8024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8992950320243835\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m logits_per_x \u001b[38;5;241m=\u001b[39m image_model(depth, seg)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits per x\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m clip_loss(logits_per_x)\n\u001b[0;32m----> 6\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for depth, seg in iter(tqdm(dataloader)):\n",
    "        depth, seg = depth.squeeze(dim=1).to(device), seg.squeeze(dim=1).to(device)\n",
    "        logits_per_x = image_model(depth, seg)['logits per x']\n",
    "        loss = clip_loss(logits_per_x)\n",
    "        tqdm.write(str(loss.item()), end='\\r')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if WANDB:\n",
    "            wandb.log({\n",
    "                'loss':loss\n",
    "            })\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save({\n",
    "            'model' : image_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, '/localhome/prateiksinha/clip/checkpoints/image_model.pt')\n",
    "if WANDB:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
